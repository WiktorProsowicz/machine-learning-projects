{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcNETI3ETW21ltSl5WG6vy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WiktorProsowicz/machine-learning-projects/blob/main/NLP/NLP_TVP_Info.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Intorduction\n",
        "The project's goal is to test some data harvesting and processing methods. This is my first project connected with ML, thus I want to focus mainly on utilising Tensorflow (especially tf's Keras) library. \n",
        "\n",
        "The main idea behind the project is to collect a fair amount of textual data from one of the divisions of polish governmental TV station (to be precise - it's online version) and create two kinds of models. \n",
        "\n",
        "One for studying the order of the words in a sequence for making article generator.\n",
        "\n",
        "The other one for generating a good word embedding that would hopefully arrange words into clusters on the basis of their close occurence in sentences."
      ],
      "metadata": {
        "id": "qzf7ggHhUb6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data harvesting & preprocessing\n"
      ],
      "metadata": {
        "id": "gabdUhFH1edi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First part of this section focuses on an algorithm that collects links to TVP Info station articles. It begins with composing a list of links from articles grid available at www.tvp.info/polska. This site seems to be dynamically generated with js its is impossible to use standard web scraping method."
      ],
      "metadata": {
        "id": "X6DXSFLf1nv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request as request\n",
        "import bs4\n",
        "import re"
      ],
      "metadata": {
        "id": "EbyXkPIYBwBN"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_PAGES = 50    # maximal number of sites like \"www.tvp.info/polska?page=x\" to visit\n",
        "links_collection = set()\n",
        "\n",
        "for page in range(1, N_PAGES + 1):\n",
        "\n",
        "    print(f\"- visiting page number {page} -\")\n",
        "\n",
        "    link_to_grid = f\"https://www.tvp.info/polska?page={page}\"\n",
        "\n",
        "    try:\n",
        "        with request.urlopen(link_to_grid) as fp:\n",
        "            html = fp.read().decode(\"utf-8\")\n",
        "\n",
        "    except:\n",
        "        print(f\"- something went wrong when accessing grid number {page} -\")\n",
        "        break\n",
        "\n",
        "    soup = bs4.BeautifulSoup(html)\n",
        "\n",
        "    # manually picked hook that occures directly before proper script tag\n",
        "    \n",
        "    info_screening_content = soup.find(\"section\", {\"class\": \"info_screening_content\"})\n",
        "    proper_script_tag = info_screening_content.fetchNextSiblings(\"script\")[0]\n",
        "\n",
        "    js_script = proper_script_tag.get_text(strip = False)\n",
        "\n",
        "    parsed = re.findall(r'\"url\" *: *\"\\\\/[0-9]*\\\\/[a-zA-z\\-]*\"', js_script)\n",
        "    viable_article_links = [element.replace(\"\\\"url\\\" : \", \"\").replace(\"\\\\\", \"\").strip(\"\\\"\") for element in parsed]\n",
        "\n",
        "    for href in viable_article_links:\n",
        "        if not href.startswith(\"https://www.tvp.info\"):\n",
        "            href = \"https://www.tvp.info\" + href\n",
        "        \n",
        "        # print(f\"- got link {href} from the grid -\")\n",
        "        links_collection.add(href)\n",
        "\n",
        "\n",
        "print(f\"-- at first collected {len(links_collection)} links --\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrOoiU4-DZ92",
        "outputId": "c50b8b8d-f5a8-4f02-c1bb-0560233d2e56"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- at first collected 826 links --\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function visits a site with use of @param link\n",
        "and collects all links from \"see also\" section"
      ],
      "metadata": {
        "id": "o6HufTpgFOXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def traverse_site(link: str):\n",
        "\n",
        "    try:\n",
        "        with request.urlopen(link) as fp:\n",
        "            html = fp.read().decode(\"utf-8\")\n",
        "\n",
        "    except:\n",
        "        print(f\"- something went wrong when traversing link {link} -\")\n",
        "        return []\n",
        "\n",
        "    soup = bs4.BeautifulSoup(html)\n",
        "    \n",
        "    double_section = soup.find(\"section\", {\"class\": \"art-detial-two-box\"})\n",
        "    triple_section = soup.find(\"section\", {\"class\": \"art-detial-three-box\"})\n",
        "\n",
        "    return_set = set()\n",
        "\n",
        "    for section in [double_section, triple_section]:\n",
        "\n",
        "        # i.e. for art-detial-two-box -- art-detial-two-box__box\n",
        "        article_classname = section.attrs[\"class\"][0] + \"__box\"\n",
        "\n",
        "        single_articles = section.find_all(\"div\", {\"class\": article_classname})\n",
        "\n",
        "        if not single_articles:\n",
        "            break\n",
        "\n",
        "        for article in single_articles:\n",
        "            main_link = article.find(\"a\", {\"class\": \"news__image\"})\n",
        "\n",
        "            if main_link:\n",
        "                href = main_link.attrs[\"href\"]\n",
        "                if not href.startswith(\"https://www.tvp.info\"):\n",
        "                    href = \"https://www.tvp.info\" + href\n",
        "                return_set.add(href)\n",
        "    \n",
        "    return return_set"
      ],
      "metadata": {
        "id": "fBHWB7YcDsgu"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) Now the part responsible for collecting as many links as possible from every article in preliminary list."
      ],
      "metadata": {
        "id": "_EuvL88WBw7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 1000  # maximal number of links to harvest\n",
        "\n",
        "visited_links = set()\n",
        "\n",
        "while len(links_collection) <= MAX_LEN:\n",
        "\n",
        "    available_links = links_collection.difference(visited_links)\n",
        "\n",
        "    if not available_links:\n",
        "        print(\"\\n--- run out of links to visit ---\\n\")\n",
        "        break\n",
        "        \n",
        "    target_link = available_links.pop()\n",
        "\n",
        "    print(f\"--- visiting {target_link} ---\")\n",
        "    harvested_links = traverse_site(target_link)\n",
        "\n",
        "    # print(f\"- harvested {harvested_links} -\")\n",
        "\n",
        "    visited_links.add(target_link)\n",
        "    links_collection.update(harvested_links)\n",
        "\n"
      ],
      "metadata": {
        "id": "qDE5od1E2F7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/tmp/tvp_links.txt\", \"w\") as target_file:\n",
        "\n",
        "    for link in links_collection:\n",
        "        target_file.write(f\"{link}\\n\")"
      ],
      "metadata": {
        "id": "Qyzb130LW1cd"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to skip the whole process, just download the file from my github."
      ],
      "metadata": {
        "id": "sBidzZfIX5aS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"\""
      ],
      "metadata": {
        "id": "al5__LWKYGCH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}